from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import f1_score, precision_recall_curve, auc, roc_curve,\
                            accuracy_score, cohen_kappa_score, roc_auc_score

import numpy as np
import pandas as pd
import itertools
import pdb

from atm.constants import *


def rank_n_accuracy(y_true, y_prob_mat, rank=5):
    """
    Compute the model's accuracy on just the n most commmon classes (I think?)
    """
    rankings = np.argsort(-y_prob_mat) # negative because we want highest value first
    rankings = rankings[:, 0:rank-1]

    num_samples = len(y_true)
    correct_sample_count = 0.0

    for i in range(num_samples):
        if y_true[i] in rankings[i, :]:
            correct_sample_count += 1.0

    return correct_sample_count / num_samples


def get_pr_roc_curves(y_true, y_pred_probs):
    """
    Compute precision/recall and receiver operating characteristic metrics for a
    binary class label.

    y_true: series of true class labels (only 1 or 0)
    y_pred_probs: series of probabilities generated by the model for the label
        class 1
    """
    results = {}
    roc = roc_curve(y_true, y_pred_probs, pos_label=1)
    results[Metrics.ROC_CURVE] = {
        'fprs': roc[0],
        'tprs': roc[1],
        'thresholds': roc[2],
    }

    pr = precision_recall_curve(y_true, y_pred_probs, pos_label=1)
    results[Metrics.PR_CURVE] = {
        'precisions': pr[0],
        'recalls': pr[1],
        'thresholds': pr[2],
    }

    return results


def get_metrics_binary(y_true, y_pred, y_pred_probs, include_curves=False):
    results = {
        Metrics.ACCURACY: accuracy_score(y_true, y_pred),
        Metrics.COHEN_KAPPA: cohen_kappa_score(y_true, y_pred),
        Metrics.F1: f1_score(y_true, y_pred),
        Metrics.ROC_AUC: np.nan,
        Metrics.AP: np.nan,
    }

    # if possible, compute PR and ROC curve metrics
    all_labels_same = len(np.unique(y_true)) == 1
    any_probs_nan = np.any(np.isnan(y_pred_probs))
    if not any_probs_nan:
        results[Metrics.AP] = average_precision_score(y_true, y_pred_probs)
        if not all_labels_same:
            results[Metrics.ROC_AUC] = roc_auc_score(y_true, y_pred_probs)

        # if necessary, compute point-by-point precision/recall and ROC curve data
        if include_curves:
            results.update(get_pr_roc_curves(y_true, y_pred_probs[:, 1]))

    return results


def get_metrics_multiclass(y_true, y_pred, y_pred_probs, rank_accuracy=False,
                           include_per_label=False, include_curves=False):
    results = {}
    results[Metrics.ACCURACY] = accuracy_score(y_true, y_pred)
    results[Metrics.COHEN_KAPPA] = cohen_kappa_score(y_true, y_pred)

    # this parameter should only be used for datasets with high-cardinality
    # labels (lots of poosible values)
    if rank_accuracy:
        results[Metrics.RANK_ACCURACY] = rank_n_accuracy(y_true=y_true,
                                                         y_prob_mat=y_pred_probs,
                                                         rank=rank)

    labels = range(y_pred_probs.shape[1])

    # create a (num_classes x num_examples) binary matrix representation of the
    # true and predicted y values.
    y_true_bin = np.zeros(y_pred_probs.shape)
    y_pred_bin = np.zeros(y_pred_probs.shape)
    for label in labels:
        y_true_bin[:, label] = (y_true == label).astype(int)
        y_pred_bin[:, label] = (y_pred == label).astype(int)

    # compute multi-label F1 metrics
    results[Metrics.F1_MICRO] = f1_score(y_true, y_pred, average='micro')
    results[Metrics.F1_MACRO] = f1_score(y_true, y_pred, average='macro')

    # if possible, compute multi-label AUC metrics
    # TODO: multi-label AP metrics?
    all_labels_same = len(np.unique(y_true)) == 1
    any_probs_nan = np.any(np.isnan(y_pred_probs))
    if all_labels_same or any_probs_nan:
        print "ROC/AUC undefined: setting AUC scores to defaults."
        results[Metrics.ROC_AUC_MICRO] = \
            METRIC_DEFAULT_SCORES[Metrics.ROC_AUC_MICRO]
        results[Metrics.ROC_AUC_MACRO] = \
            METRIC_DEFAULT_SCORES[Metrics.ROC_AUC_MACRO]
    else:
        try:
            results[Metrics.ROC_AUC_MICRO] = roc_auc_score(y_true_bin, y_pred_probs,
                                                           average='micro')
            results[Metrics.ROC_AUC_MACRO] = roc_auc_score(y_true_bin, y_pred_probs,
                                                           average='macro')
        except:
            pdb.set_trace()

    # labelwise controls whether to compute separate metrics for each posisble label
    if include_per_label or include_curves:
        results['labelwise'] = {}
        # for each possible class, generate F1, precision-recall, and ROC scores
        # using the binary metrics function.
        for label in labels:
            label_pred_probs = np.column_stack((1 - y_pred_probs[:, label],
                                                y_pred_probs[:, label]))
            label_res = get_metrics_binary(y_true=y_true_bin[:, label],
                                           y_pred=y_pred_bin[:, label],
                                           y_pred_probs=label_pred_probs,
                                           include_curves=include_curves)
            results['labelwise'][label] = label_res

    return results


def test_pipeline(pipeline, X, y, binary, **kwargs):
    if binary:
        get_metrics = get_metrics_binary
    else:
        get_metrics = get_metrics_multiclass

    # run the test data through the trained pipeline
    y_pred = pipeline.predict(X)

    # if necessary (i.e. if a pipeline does not produce probability scores by
    # default), use class distance scores in lieu of probability scores
    method = pipeline.steps[-1][0]
    if method in ['sgd', 'pa']:
        if binary:
            class_1_distance = pipeline.decision_function(X)
            class_0_distance = -class_1_distance
            y_pred_probs = np.column_stack((class_0_distance, class_1_distance))
        else:
            y_pred_probs = pipeline.decision_function(X)
    else:
        y_pred_probs = pipeline.predict_proba(X)

    return get_metrics(y, y_pred, y_pred_probs, **kwargs)


def cross_validate_pipeline(pipeline, X, y, binary=True,
                            n_folds=N_FOLDS_DEFAULT, **kwargs):
    """
    Compute metrics for each of `n_folds` folds of the training data in (X, y).

    pipeline: the sklearn Pipeline to train and test
    X: feature matrix
    y: series of labels corresponding to rows in X
    binary: whether the label is binary or multi-ary
    """
    if binary:
        metrics = METRICS_BINARY
    else:
        metrics = METRICS_MULTICLASS

    df = pd.DataFrame(columns=metrics)
    results = []

    skf = StratifiedKFold(n_splits=n_folds)
    skf.get_n_splits(X, y)

    for train_index, test_index in skf.split(X, y):
        pipeline.fit(X[train_index], y[train_index])
        split_results = test_pipeline(pipeline=pipeline,
                                      X=X[test_index],
                                      y=y[test_index],
                                      binary=binary, **kwargs)
        df = df.append([{m: split_results.get(m) for m in metrics}])
        results.append(split_results)

    return df, results

